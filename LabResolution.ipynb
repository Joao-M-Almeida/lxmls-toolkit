{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_be_profiled(n):\n",
    "    if n != 0:\n",
    "        return n * to_be_profiled(n-1)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%prun to_be_profiled(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cProfile.run('to_be_profiled(9)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ex 0.6\n",
    "A = np.arange(100)\n",
    "C = np.cos(A) \n",
    "print C.ptp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ex 0.7\n",
    "i = np.linspace(0, 0.999, 1000)\n",
    "np.sum((i ** 2)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ex 0.10\n",
    "import lxmls.readers.galton as galton\n",
    "galton_data = galton.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "print \"Father avg {0}\".format(galton_data[:,0].mean())\n",
    "print \"Son avg {0}\".format(galton_data[:,1].mean())\n",
    "\n",
    "print \"Father std {0}\".format(galton_data[:,0].std())\n",
    "print \"Son std {0}\".format(galton_data[:,1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(galton_data[:,0].flatten(), bins=20, alpha=0.3)\n",
    "plt.hist(galton_data[:,1].flatten(), bins=20, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(galton_data[:,0],galton_data[:,1], 'ro')\n",
    "plt.xticks(xrange(60,80))\n",
    "plt.yticks(xrange(60,80))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomized_data = galton_data + np.random.randn(galton_data.shape[0],2)\n",
    "\n",
    "plt.plot(randomized_data[:,0],randomized_data[:,1], 'ro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ex 0.12\n",
    "def get_y(x):\n",
    "    return (x+2)**2 - 16*np.exp(-((x-2)**2))\n",
    "\n",
    "def get_grad(x):\n",
    "    return (2*x+4)-16*(-2*x + 4)*np.exp(-((x-2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-8,8,0.001)\n",
    "y = get_y(x)\n",
    "plt.plot(x,y, 'r')\n",
    "plt.plot(x,get_grad(x),'b')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(start_x,func,grad, step): # Precision of the solution\n",
    "    prec = 0.1\n",
    "    #Use a fixed small step size \n",
    "    step_size = step\n",
    "    alpha = 0.9\n",
    "    #max iterations\n",
    "    max_iter = 100\n",
    "    x_new = start_x\n",
    "    res = []\n",
    "    for i in xrange(max_iter):\n",
    "        x_old = x_new\n",
    "        #Use beta egual to -1 for gradient descent\n",
    "        x_new = x_old - step_size * grad(x_new) \n",
    "        f_x_new = func(x_new)\n",
    "        f_x_old = func(x_old) \n",
    "        res.append([x_new,f_x_new]) \n",
    "        if(abs(f_x_new - f_x_old) < prec):\n",
    "            print \"change in function values too small, leaving\"\n",
    "            return np.array(res)\n",
    "        step_size *= alpha \n",
    "    print \"exceeded maximum number of iterations, leaving\" \n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_0 = 8\n",
    "res = gradient_descent(x_0,get_y,get_grad)\n",
    "plt.plot(res[:,0],res[:,1],'+')\n",
    "plt.plot(x,y, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([1,1])\n",
    "y = galton_data[:,1]\n",
    "x = np.vstack([galton_data[:,0], np.ones(len(galton_data[:,0]))]).T\n",
    "\n",
    "def error(w):\n",
    "    y_est = x.dot(w)\n",
    "    return np.sum((y_est-y)**2)\n",
    "\n",
    "def error_grad(w):\n",
    "    return sum(2*(x.dot(w)).dot(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.lstsq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = [0.643,23.99]\n",
    "error_grad(w)\n",
    "2*(x.dot(w)).dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_descent([1,1],error, error_grad, 0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.sentiment_reader as srs \n",
    "scr = srs.SentimentCorpus(\"books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.multinomial_naive_bayes as mnbb\n",
    "\n",
    "mnb = mnbb.MultinomialNaiveBayes()\n",
    "params_nb_sc = mnb.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = mnb.test(scr.train_X,params_nb_sc)\n",
    "acc_train = mnb.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = mnb.test(scr.test_X,params_nb_sc)\n",
    "acc_test = mnb.evaluate(scr.test_y, y_pred_test)\n",
    "\n",
    "print \"Multinomial Naive Bayes Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.simple_data_set as sds\n",
    "sd = sds.SimpleDataSet(nr_examples=100, g1 = [[-1,-1],1], g2 = [[1,1],1], balance=0.5, split=[0.5,0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.perceptron as percc\n",
    "perc = percc.Perceptron(nr_epochs=21)\n",
    "params_perc_sd, w_lst = perc.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = perc.test(sd.train_X,params_perc_sd)\n",
    "acc_train = perc.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = perc.test(sd.test_X,params_perc_sd)\n",
    "acc_test = perc.evaluate(sd.test_y, y_pred_test)\n",
    "print \"Perceptron Simple Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,axis = sd.plot_data()\n",
    "fig,axis = sd.add_line(fig,axis,params_perc_sd,\"Perceptron\",\"blue\")\n",
    "for i in xrange(len(w_lst)):\n",
    "    if i % 5 == 0:\n",
    "        fig,axis = sd.add_line(fig,axis,params_perc_sd,\"Percept %d\" % (i),\"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.perceptron as percc\n",
    "perc = percc.Perceptron(nr_epochs=20)\n",
    "params_perc_scr, w_lst = perc.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = perc.test(scr.train_X,params_perc_scr)\n",
    "acc_train = perc.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = perc.test(scr.test_X,params_perc_scr)\n",
    "acc_test = perc.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Perceptron Amazon Sentient Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.mira as mirac\n",
    "mira = mirac.Mira()\n",
    "mira.regularizer = 0.5 # This is lambda\n",
    "params_mira_sd = mira.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = mira.test(sd.train_X,params_mira_sd)\n",
    "acc_train = mira.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = mira.test(sd.test_X,params_mira_sd)\n",
    "acc_test = mira.evaluate(sd.test_y, y_pred_test)\n",
    "print \"Mira Simple Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test) \n",
    "fig,axis = sd.plot_data()\n",
    "fig,axis = sd.add_line(fig,axis,params_mira_sd,\"Mira\",\"green\")\n",
    "params_mira_sc = mira.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = mira.test(scr.train_X,params_mira_sc)\n",
    "acc_train = mira.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = mira.test(scr.test_X,params_mira_sc)\n",
    "acc_test = mira.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Mira Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.max_ent_batch as mebc\n",
    "me_lbfgs = mebc.MaxEntBatch()\n",
    "me_lbfgs.regularizer = 1.0\n",
    "params_meb_sd = me_lbfgs.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = me_lbfgs.test(sd.train_X,params_meb_sd)\n",
    "acc_train = me_lbfgs.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = me_lbfgs.test(sd.test_X,params_meb_sd)\n",
    "acc_test = me_lbfgs.evaluate(sd.test_y, y_pred_test)\n",
    "print \"Max-Ent batch Simple Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test\n",
    "    )\n",
    "fig,axis = sd.plot_data()\n",
    "fig,axis = sd.add_line(fig,axis,params_meb_sd,\"Max-Ent-Batch\",\"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params_meb_sc = me_lbfgs.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = me_lbfgs.test(scr.train_X,params_meb_sc)\n",
    "acc_train = me_lbfgs.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = me_lbfgs.test(scr.test_X,params_meb_sc)\n",
    "acc_test = me_lbfgs.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Max-Ent Batch Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.max_ent_online as meoc\n",
    "me_sgd = meoc.MaxEntOnline()\n",
    "me_sgd.regularizer = 1.0\n",
    "params_meo_sc = me_sgd.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = me_sgd.test(scr.train_X,params_meo_sc)\n",
    "acc_train = me_sgd.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = me_sgd.test(scr.test_X,params_meo_sc)\n",
    "acc_test = me_sgd.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Max-Ent Online Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.classifiers.svm as svmc\n",
    "svm = svmc.SVM()\n",
    "svm.regularizer = 1.0 # This is lambda\n",
    "params_svm_sd = svm.train(sd.train_X,sd.train_y)\n",
    "y_pred_train = svm.test(sd.train_X,params_svm_sd)\n",
    "acc_train = svm.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = svm.test(sd.test_X,params_svm_sd)\n",
    "acc_test = svm.evaluate(sd.test_y, y_pred_test)\n",
    "\n",
    "\n",
    "print \"SVM Online Simple Dataset Accuracy train: %f test: %f\"%(acc_train,acc_test)\n",
    "fig,axis = sd.add_line(fig,axis,params_svm_sd,\"SVM\",\"orange\")\n",
    "params_svm_sc = svm.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = svm.test(scr.train_X,params_svm_sc)\n",
    "acc_train = svm.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = svm.test(scr.test_X,params_svm_sc)\n",
    "acc_test = svm.evaluate(scr.test_y, y_pred_test)\n",
    "print \"SVM Online Amazon Sentiment Accuracy train: %f test: %f\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from  lxmls.run_all_classifiers import run_all_classifiers\n",
    "\n",
    "run_all_classifiers(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.simple_sequence as ssr \n",
    "simple = ssr.SimpleSequence()\n",
    "print simple.train\n",
    "print simple.test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sequence in simple.train.seq_list: \n",
    "    print sequence\n",
    "print \"activity labels:\"    \n",
    "for sequence in simple.train.seq_list: \n",
    "    print sequence.x\n",
    "# walk 0\n",
    "# shop 1\n",
    "# clean 2\n",
    "print \"weather labels:\"    \n",
    "for sequence in simple.train.seq_list: \n",
    "    print sequence.y\n",
    "# sunny 1\n",
    "# rainy 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict) \n",
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "print \"Initial Probabilities:\", hmm.initial_probs\n",
    "print \"Initial counts:\", hmm.initial_counts\n",
    "\n",
    "print \"Transition Probabilities:\\n\", hmm.transition_probs\n",
    "print \"transition counts:\\n\", hmm.transition_counts\n",
    "\n",
    "print \"Final Probabilities:\", hmm.final_probs\n",
    "print \"final counts:\", hmm.final_counts\n",
    "\n",
    "print \"Emission Probabilities\\n\", hmm.emission_probs\n",
    "print \"emission counts:\\n\", hmm.emission_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Initial scores:\\n\", initial_scores\n",
    "print \"Transition scores:\\n\", transition_scores\n",
    "print \"Final scores:\\n\", final_scores\n",
    "print \"Emission scores:\\n\", emission_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood, forward = hmm.decoder.run_forward(initial_scores, transition_scores, final_scores, emission_scores)\n",
    "print 'Log-Likelihood =', log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_likelihood, backward = hmm.decoder.run_backward(initial_scores, transition_scores, final_scores, emission_scores)\n",
    "print 'Log-Likelihood =', log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[0])\n",
    "state_posteriors, _, _ = hmm.compute_posteriors(initial_scores, transition_scores, final_scores, emission_scores)\n",
    " \n",
    "print state_posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[0]) \n",
    "print \"Prediction test 0:\\t\", y_pred\n",
    "print \"Truth test 0:\\t\\t\", simple.test.seq_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = hmm.posterior_decode(simple.test.seq_list[1]) \n",
    "print \"Prediction test 1:\\t\", y_pred\n",
    "print \"Truth test 1:\\t\\t\", simple.test.seq_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.simple_sequence as ssr \n",
    "simple = ssr.SimpleSequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict) \n",
    "hmm.train_supervised(simple.train, smoothing=0.1)\n",
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[0])\n",
    "print \"Viterbi decoding Prediction test 0 with smoothing:\\n\", y_pred, score\n",
    "print \"Truth test 0:\", simple.test.seq_list[0]\n",
    "y_pred, score = hmm.viterbi_decode(simple.test.seq_list[1])\n",
    "\n",
    "print \"Viterbi decoding Prediction test 1 with smoothing:\\n\", y_pred, score\n",
    "print \"Truth test 1:\", simple.test.seq_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.pos_corpus as pcc\n",
    "%matplotlib inline\n",
    "corpus = pcc.PostagCorpus()\n",
    "train_seq = corpus.read_sequence_list_conll(\"data/train-02-21.conll\",max_sent_len=15, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(\"data/test-23.conll\",max_sent_len=15, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(\"data/dev-22.conll\",max_sent_len=15,max_nr_sent=1000)\n",
    "hmm = hmmc.HMM(corpus.word_dict, corpus.tag_dict)\n",
    "hmm.train_supervised(train_seq)\n",
    "hmm.print_transition_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viterbi_pred_train = hmm.viterbi_decode_corpus(train_seq)\n",
    "posterior_pred_train = hmm.posterior_decode_corpus(train_seq) \n",
    "eval_viterbi_train = hmm.evaluate_corpus(train_seq, viterbi_pred_train) \n",
    "eval_posterior_train = hmm.evaluate_corpus(train_seq, posterior_pred_train) \n",
    "print \"Train Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(eval_posterior_train,eval_viterbi_train)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq) \n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq) \n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq,viterbi_pred_test) \n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq,posterior_pred_test) \n",
    "print \"Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(eval_posterior_test,eval_viterbi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_smoothing = hmm.pick_best_smoothing(train_seq, dev_seq, [10,1,0.1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hmm.train_supervised(train_seq, smoothing=best_smoothing)\n",
    "viterbi_pred_test = hmm.viterbi_decode_corpus(test_seq)\n",
    "posterior_pred_test = hmm.posterior_decode_corpus(test_seq)\n",
    "eval_viterbi_test = hmm.evaluate_corpus(test_seq, viterbi_pred_test) \n",
    "eval_posterior_test = hmm.evaluate_corpus(test_seq, posterior_pred_test)\n",
    "print \"Best Smoothing %f -- Test Set Accuracy: Posterior Decode %.3f, Viterbi Decode: %.3f\"%(\n",
    "    best_smoothing,eval_posterior_test,eval_viterbi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.confusion_matrix as cm\n",
    "import matplotlib.pyplot as plt\n",
    "confusion_matrix = cm.build_confusion_matrix(\n",
    "    test_seq.seq_list, viterbi_pred_test, len(corpus.tag_dict), hmm.get_num_states())\n",
    "cm.plot_confusion_bar_graph(confusion_matrix, corpus.tag_dict, xrange(hmm.get_num_states()), 'Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.sequences.structured_perceptron as spc \n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"CRF Exercise\"\n",
    "corpus = pcc.PostagCorpus()\n",
    "train_seq = corpus.read_sequence_list_conll(\"data/train-02-21.conll\",max_sent_len=10, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(\"data/test-23.conll\",max_sent_len=10, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(\"data/dev-22.conll\",max_sent_len=10,max_nr_sent=1000)\n",
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "print \"CRF - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train, eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = exfc.ExtendedFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "print \"CRF - Extended Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(eval_train, eval_dev,eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(50):\n",
    "    if ((train_seq[i].y != pred_train[i].y).any()):\n",
    "        print train_seq[i]\n",
    "        print pred_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(20):\n",
    "    if ((dev_seq[i].y != pred_dev[i].y).any()):\n",
    "        print dev_seq[i]\n",
    "        print pred_dev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(20):\n",
    "    if ((test_seq[i].y != pred_test[i].y).any()):\n",
    "        print test_seq[i]\n",
    "        print pred_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "print \"Perceptron Exercise\"\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 20\n",
    "sp.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)\n",
    "eval_train = sp.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = sp.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = sp.evaluate_corpus(test_seq, pred_test)\n",
    "print \"Structured Perceptron - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%( eval_train,eval_dev,eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_mapper = exfc.ExtendedFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 20\n",
    "sp.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)\n",
    "eval_train = sp.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = sp.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = sp.evaluate_corpus(test_seq, pred_test)\n",
    "print \"Structured Perceptron - Extended Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\"%(\n",
    "    eval_train,eval_dev,eval_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('.' )\n",
    "import lxmls.parsing.dependency_parser as depp\n",
    "dp = depp.DependencyParser()\n",
    "dp.read_data(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.features.use_lexical = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10) \n",
    "dp.test()\n",
    "dp.features.use_distance = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10) \n",
    "dp.test()\n",
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.train_crf_sgd(10, 0.01, 0.1)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser() \n",
    "dp.features.use_lexical = True \n",
    "dp.features.use_distance = True \n",
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"english\") \n",
    "dp.projective = True\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.readers.sentiment_reader as srs \n",
    "scr = srs.SentimentCorpus(\"books\")\n",
    "train_x = scr.train_X.T\n",
    "train_y = scr.train_y[:, 0]\n",
    "test_x = scr.test_X.T\n",
    "test_y = scr.test_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural network modules\n",
    "import lxmls.deep_learning.mlp as dl \n",
    "import lxmls.deep_learning.sgd as sgd # Model parameters\n",
    "geometry = [train_x.shape[0], 20, 2] \n",
    "actvfunc = ['sigmoid',  'softmax']\n",
    "# Instantiate model\n",
    "mlp = dl.NumpyMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_iter = 5\n",
    "bsize  = 5\n",
    "lrate  = 0.01\n",
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "print \"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\" % (geometry, acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = test_x             # Test set\n",
    "W1, b1 = mlp.params[:2]     # Weights and bias of fist layer\n",
    "z1= np.dot(W1, x) + b1 # Linear transformation\n",
    "tilde_z1 = 1/(1+np.exp(-z1))  # Non-linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano code.\n",
    "# NOTE: We use undescore to denote symbolic equivalents to Numpy variables. # This is no Python convention!.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "_x = T.matrix('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_W1 = theano.shared(value=W1, name='W1', borrow=True)\n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_z1            = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1      = T.nnet.sigmoid(_z1)\n",
    "# Keep in mind that naming variables is useful when debugging\n",
    "_z1.name       = 'z1'\n",
    "_tilde_z1.name = 'tilde_z1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perceptron computation graph\n",
    "theano.printing.debugprint(_tilde_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer1 = theano.function([_x], _tilde_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check Numpy and Theano match\n",
    "np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train_x.astype(theano.config.floatX)\n",
    "train_y = train_y.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_a = dl.NumpyMLP(geometry, actvfunc)\n",
    "mlp_b = dl.TheanoMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.allclose(mlp_a.forward(test_x), mlp_b.forward(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2, b2 = mlp.params[2:] # Weights and bias of second (and last!) layer\n",
    "# Second layer symbolic variables\n",
    "_W2 = theano.shared(value=W2, name='W2', borrow=True)\n",
    "_b2 = theano.shared(value=b2, name='b2', borrow=True, broadcastable=(False, True)) # Second layer symbolic expressions\n",
    "_z2       = T.dot(_W2, _tilde_z1) + _b2\n",
    "_tilde_z2 = T.nnet.softmax(_z2.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_y = T.ivector('y')\n",
    "_F = -T.mean(T.log(_tilde_z2[_y, T.arange(_y.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_nabla_F = T.grad(_F, _W1)\n",
    "nabla_F = theano.function([_x, _y], _nabla_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_c   = dl.TheanoMLP(geometry, actvfunc)\n",
    "_x      = T.matrix('x')\n",
    "_y      = T.ivector('y')\n",
    "_F      = mlp_c._cost(_x, _y)\n",
    "# SGD update rule\n",
    "updates = [(par, par - lrate*T.grad(_F, par)) for par in mlp_c.params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_up = theano.function([_x, _y], _F, updates=updates)\n",
    "n_batch = int(np.ceil(float(train_x.shape[1])/bsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Model\n",
    "geometry = [train_x.shape[0], 20, 2] \n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "# Numpy MLP\n",
    "mlp_a = dl.NumpyMLP(geometry, actvfunc)\n",
    "init_t = time.clock()\n",
    "sgd.SGD_train(mlp_a, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y)) \n",
    "print \"\\nNumpy version took %2.2f sec\" % (time.clock() - init_t)\n",
    "acc_train = sgd.class_acc(mlp_a.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_a.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)\n",
    "# Theano grads\n",
    "mlp_b = dl.TheanoMLP(geometry, actvfunc)\n",
    "init_t = time.clock()\n",
    "sgd.SGD_train(mlp_b, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y)) \n",
    "print \"\\nCompiled gradient version took %2.2f sec\" % (time.clock() - init_t) \n",
    "acc_train = sgd.class_acc(mlp_b.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_b.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)\n",
    "# Theano batch update\n",
    "init_t = time.clock()\n",
    "sgd.SGD_train(mlp_c, n_iter, batch_up=batch_up, n_batch=n_batch, bsize=bsize,\n",
    "train_set=(train_x, train_y))\n",
    "print \"\\nTheano compiled batch update version took %2.2f\" % (time.clock() - init_t) \n",
    "acc_train = sgd.class_acc(mlp_c.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_c.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\"%(acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Part-of-Speech data\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "corpus = pcc.PostagCorpus()\n",
    "train_seq = corpus.read_sequence_list_conll(\"data/train-02-21.conll\", max_sent_len=15,\n",
    "    max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(\"data/test-23.conll\", max_sent_len=15,\n",
    "    max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(\"data/dev-22.conll\", max_sent_len=15,\n",
    "    max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Redo indices\n",
    "train_seq, test_seq, dev_seq = pcc.compacify(train_seq, test_seq, dev_seq, theano=True) \n",
    "# Get number of words and tags in the corpus\n",
    "nr_words = len(train_seq.x_dict)\n",
    "nr_tags = len(train_seq.y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.deep_learning.rnn as rnns\n",
    "#reload(rnns)\n",
    "# RNN configuration\n",
    "SEED = 1234 # Random seed to initialize weigths \n",
    "emb_size = 50 # Size of word embeddings \n",
    "hidden_size = 20 # size of hidden layer\n",
    "# RNN\n",
    "np_rnn = rnns.NumpyRNN(nr_words, emb_size, hidden_size, nr_tags, seed=SEED)\n",
    "# Example sentence\n",
    "x0 = train_seq[0].x\n",
    "y0 = train_seq[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "p_y, y_rnn, h, z1, x = np_rnn.forward(x0, all_outputs=True) \n",
    "# Gradients\n",
    "numpy_rnn_gradients = np_rnn.grads(x0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T \n",
    "theano.config.optimizer='None'\n",
    "def square(x): \n",
    "    return x**2\n",
    "# Python\n",
    "def np_square_n_steps(nr_steps): \n",
    "    out = []\n",
    "    for n in np.arange(nr_steps): \n",
    "        out.append(square(n))\n",
    "    return np.array(out)\n",
    "\n",
    "# Theano\n",
    "nr_steps = T.lscalar('nr_steps')\n",
    "h, _ = theano.scan(fn=square, sequences=T.arange(nr_steps))\n",
    "th_square_n_steps = theano.function([nr_steps], h)\n",
    "# Compare both\n",
    "print np_square_n_steps(15) \n",
    "print th_square_n_steps(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "nr_states = 3\n",
    "nr_steps = 5\n",
    "# Transition matrix\n",
    "A = np.abs(np.random.randn(nr_states, nr_states)) \n",
    "A = A/A.sum(0, keepdims=True)\n",
    "# Initial state\n",
    "s0 = np.zeros(nr_states)\n",
    "s0[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numpy version\n",
    "def np_markov_step(s_tm1): \n",
    "    s_t = np.dot(s_tm1, A.T) \n",
    "    return s_t\n",
    "def np_markov_chain(nr_steps, A, s0):\n",
    "# Pre-allocate space\n",
    "    s = np.zeros((nr_steps+1, nr_states)) \n",
    "    s[0, :] = s0\n",
    "    for t in np.arange(nr_steps):\n",
    "        s[t+1, :] = np_markov_step(s[t, :]) \n",
    "    return s\n",
    "np_markov_chain(nr_steps, A, s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theano version\n",
    "# Store variables as shared variables\n",
    "th_A = theano.shared(A, name='A', borrow=True) \n",
    "th_s0 = theano.shared(s0, name='s0', borrow=True) \n",
    "# Symbolic variable for the number of steps \n",
    "th_nr_steps = T.lscalar('nr_steps')\n",
    "def th_markov_step(s_tm1):\n",
    "    s_t = T.dot(s_tm1, th_A.T)\n",
    "    # Remember to name variables \n",
    "    s_t.name = 's_t'\n",
    "    return s_t\n",
    "s, _ = theano.scan(th_markov_step, outputs_info=[dict(initial=th_s0)],n_steps=th_nr_steps)\n",
    "th_markov_chain = theano.function([th_nr_steps], T.concatenate((th_s0[None, :], s), 0))\n",
    "th_markov_chain(nr_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "rnn = rnns.RNN(nr_words, emb_size, hidden_size, nr_tags, seed=SEED)\n",
    "# Compile the forward pass function\n",
    "x = T.ivector('x')\n",
    "th_forward = theano.function([x], rnn._forward(x).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert np.allclose(th_forward(x0), np_rnn.forward(x0)), \"Numpy and Theano forward pass differ!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile function returning the list of gradients\n",
    "x = T.ivector('x')     # Input words\n",
    "y = T.ivector('y')     # gold tags\n",
    "p_y = rnn._forward(x)\n",
    "cost = -T.mean(T.log(p_y)[T.arange(y.shape[0]), y])\n",
    "grads_fun = theano.function([x, y], [T.grad(cost, par) for par in rnn.param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare numpy and theano gradients\n",
    "theano_rnn_gradients = grads_fun(x0, y0) \n",
    "for n in range(len(theano_rnn_gradients)):\n",
    "    assert np.allclose(numpy_rnn_gradients[n], theano_rnn_gradients[n]), \"Numpy and Theano gradients differ in step n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "lrate = 0.5 # Learning rate\n",
    "n_iter = 5 # Number of iterations\n",
    "# Get list of SGD batch update rule for each parameter\n",
    "updates = [(par, par - lrate*T.grad(cost, par)) for par in rnn.param] # compile\n",
    "rnn_prediction = theano.function([x], T.argmax(p_y, 1)) \n",
    "rnn_batch_update = theano.function([x, y], cost, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "nr_words = sum([len(seq.x) for seq in train_seq]) \n",
    "for i in range(n_iter):\n",
    "    # Training\n",
    "    cost = 0\n",
    "    errors = 0\n",
    "    for n, seq in enumerate(train_seq):\n",
    "        cost += rnn_batch_update(seq.x, seq.y)\n",
    "        errors += sum(rnn_prediction(seq.x) != seq.y)\n",
    "    acc_train = 100*(1-errors*1./nr_words)\n",
    "    print \"Epoch %d: Train cost %2.2f Acc %2.2f %%\" % (i+1, cost, acc_train), # Evaluation\n",
    "    errors = 0\n",
    "    for n, seq in enumerate(dev_seq):\n",
    "        errors += sum(rnn_prediction(seq.x) != seq.y) \n",
    "    acc_dev = 100*(1-errors*1./nr_words)\n",
    "    print \" Devel Acc %2.2f %%\" % acc_dev \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embeddings Path\n",
    "import lxmls.deep_learning.embeddings as emb \n",
    "import os\n",
    "reload(emb)\n",
    "if not os.path.isfile('EMBEDDINGS'):\n",
    "    emb.download_embeddings('senna_50', \"data/senna_50\")\n",
    "E = emb.extract_embeddings(\"data/senna_50\", train_seq.x_dict)\n",
    "# Reset model to remove the effect of training\n",
    "rnn = rnns.reset_model(rnn, seed=SEED)\n",
    "# Set the embedding layer to the pre-trained values\n",
    "rnn.param[0].set_value(E.astype(theano.config.floatX))\n",
    "# Now re-run SGD training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate LSTM\n",
    "lstm = rnns.LSTM(nr_words, emb_size, hidden_size, nr_tags)\n",
    "# Compile prediction and bacth update functions\n",
    "lstm_prediction = theano.function([x], T.argmax(lstm._forward(x), 1))\n",
    "lstm_cost = -T.mean(T.log(lstm._forward(x))[T.arange(y.shape[0]), y])\n",
    "# Get list of SGD batch update rule for each parameter\n",
    "lstm_updates = [(par, par - lrate*T.grad(lstm_cost, par)) for par in lstm.param] # compile\n",
    "lstm_batch_update = theano.function([x, y], lstm_cost, updates=lstm_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_words = sum([len(seq.x) for seq in train_seq]) \n",
    "for i in range(n_iter):\n",
    "    # Training\n",
    "    cost = 0\n",
    "    errors = 0\n",
    "    for n, seq in enumerate(train_seq):\n",
    "        cost += lstm_batch_update(seq.x, seq.y)\n",
    "        errors += sum(lstm_prediction(seq.x) != seq.y)\n",
    "    acc_train = 100*(1-errors*1./nr_words)\n",
    "    print \"Epoch %d: Train cost %2.2f Acc %2.2f %%\" % (i+1, cost, acc_train), \n",
    "    # Evaluation:\n",
    "    errors = 0\n",
    "    for n, seq in enumerate(dev_seq):\n",
    "        errors += sum(lstm_prediction(seq.x) != seq.y) \n",
    "        acc_dev = 100*(1-errors*1./nr_words)\n",
    "    print \" Devel Acc %2.2f %%\" % acc_dev \n",
    "    sys.stdout.flush()\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
